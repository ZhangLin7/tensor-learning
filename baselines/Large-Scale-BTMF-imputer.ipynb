{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Temporal Matrix Factorization\n",
    "\n",
    "**Published**: October 8, 2019\n",
    "\n",
    "**Revised**: October 8, 2020\n",
    "\n",
    "**Author**: Xinyu Chen [[**GitHub homepage**](https://github.com/xinychen)]\n",
    "\n",
    "**Download**: This Jupyter notebook is at our GitHub repository. If you want to evaluate the code, please download the notebook from the [**transdim**](https://github.com/xinychen/transdim/blob/master/imputer/BTMF.ipynb) repository.\n",
    "\n",
    "This notebook shows how to implement the Bayesian Temporal Matrix Factorization (BTMF), a fully Bayesian matrix factorization model, on some real-world data sets. To overcome the missing data problem in multivariate time series, BTMF takes into account both low-rank matrix structure and time series autoregression. For an in-depth discussion of BTMF, please see [1].\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=\"black\">\n",
    "<b>[1]</b> Xinyu Chen, Lijun Sun (2019). <b>Bayesian temporal factorization for multidimensional time series prediction</b>. arXiv:1910.06366. <a href=\"https://arxiv.org/pdf/1910.06366.pdf\" title=\"PDF\"><b>[PDF]</b></a> \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Temporal Matrix Factorization Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Model Specification\n",
    "\n",
    "Following the general Bayesian probabilistic matrix factorization models (e.g., BPMF proposed by [Salakhutdinov & Mnih, 2008](https://www.cs.toronto.edu/~amnih/papers/bpmf.pdf)), we assume that each observed entry in $Y$ follows a Gaussian distribution with precision $\\tau$:\n",
    "\\begin{equation}\n",
    "y_{i,t}\\sim\\mathcal{N}\\left(\\boldsymbol{w}_i^\\top\\boldsymbol{x}_t,\\tau^{-1}\\right),\\quad \\left(i,t\\right)\\in\\Omega.\n",
    "\\label{btmf_equation3}\n",
    "\\end{equation}\n",
    "\n",
    "On the spatial dimension, we use a simple Gaussian factor matrix without imposing any dependencies explicitly:\n",
    "\\begin{equation}\n",
    "\\boldsymbol{w}_i\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_{w},\\Lambda_w^{-1}\\right),\n",
    "\\end{equation}\n",
    "and we place a conjugate Gaussian-Wishart prior on the mean vector and the precision matrix:\n",
    "\\begin{equation}\n",
    "\\boldsymbol{\\mu}_w | \\Lambda_w \\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_0,(\\beta_0\\Lambda_w)^{-1}\\right),\\Lambda_w\\sim\\mathcal{W}\\left(W_0,\\nu_0\\right),\n",
    "\\end{equation}\n",
    "where $\\boldsymbol{\\mu}_0\\in \\mathbb{R}^{R}$ is a mean vector, $\\mathcal{W}\\left(W_0,\\nu_0\\right)$ is a Wishart distribution with a $R\\times R$ scale matrix $W_0$ and $\\nu_0$ degrees of freedom.\n",
    "\n",
    "In modeling the temporal factor matrix $X$, we re-write the VAR process as:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{x}_{t}&\\sim\\begin{cases}\n",
    "\\mathcal{N}\\left(\\boldsymbol{0},I_R\\right),&\\text{if $t\\in\\left\\{1,2,...,h_d\\right\\}$}, \\\\\n",
    "\\mathcal{N}\\left(A^\\top \\boldsymbol{v}_{t},\\Sigma\\right),&\\text{otherwise},\\\\\n",
    "\\end{cases}\\\\\n",
    "\\end{aligned}\n",
    "\\label{btmf_equation5}\n",
    "\\end{equation}\n",
    "\n",
    "Since the mean vector is defined by VAR, we need to place the conjugate matrix normal inverse Wishart (MNIW) prior on the coefficient matrix $A$ and the covariance matrix $\\Sigma$ as follows,\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "A\\sim\\mathcal{MN}_{(Rd)\\times R}\\left(M_0,\\Psi_0,\\Sigma\\right),\\quad\n",
    "\\Sigma \\sim\\mathcal{IW}\\left(S_0,\\nu_0\\right), \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where the probability density function for the $Rd$-by-$R$ random matrix $A$ has the form:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&p\\left(A\\mid M_0,\\Psi_0,\\Sigma\\right) \\\\\n",
    "=&\\left(2\\pi\\right)^{-R^2d/2}\\left|\\Psi_0\\right|^{-R/2}\\left|\\Sigma\\right|^{-Rd/2} \\\\\n",
    "&\\times \\exp\\left(-\\frac{1}{2}\\text{tr}\\left[\\Sigma^{-1}\\left(A-M_0\\right)^{\\top}\\Psi_{0}^{-1}\\left(A-M_0\\right)\\right]\\right), \\\\\n",
    "\\end{aligned}\n",
    "\\label{mnpdf}\n",
    "\\end{equation}\n",
    "where $\\Psi_0\\in\\mathbb{R}^{(Rd)\\times (Rd)}$ and $\\Sigma\\in\\mathbb{R}^{R\\times R}$ are played as covariance matrices.\n",
    "\n",
    "For the only remaining parameter $\\tau$, we place a Gamma prior  $\\tau\\sim\\text{Gamma}\\left(\\alpha,\\beta\\right)$ where $\\alpha$ and $\\beta$ are the shape and rate parameters, respectively. \n",
    "\n",
    "The above specifies the full generative process of BTMF, and we could also see the Bayesian graphical model shown in Figure 4. Several parameters are introduced to define the prior distributions for hyperparameters, including $\\boldsymbol{\\mu}_{0}$, $W_0$, $\\nu_0$, $\\beta_0$, $\\alpha$, $\\beta$, $M_0$, $\\Psi_0$, and $S_0$. These parameters need to provided in advance when training the model. However, it should be noted that the specification of these parameters has little impact on the final results, as the training data will play a much more important role in defining the posteriors of the hyperparameters.\n",
    "\n",
    "<img src=\"../images/btmf_net.png\" alt=\"drawing\" width=\"450\"/>\n",
    "\n",
    "> **Figure 4**: An overview graphical model of BTMF (time lag set: $\\left\\{1,2,...,d\\right\\}$). The shaded nodes ($y_{i,t}$) are the observed data in $\\Omega$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Model Inference\n",
    "\n",
    "Given the complex structure of BTMF, it is intractable to write down the posterior distribution. Here we rely on the MCMC technique for Bayesian learning. In detail, we introduce a Gibbs sampling algorithm by deriving the full conditional distributions for all parameters and hyperparameters. Thanks to the use of conjugate priors in Figure 4, we can actually write down all the conditional distributions analytically. Below we summarize the Gibbs sampling procedure.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Sampling Factor Matrix $W$ and Its Hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For programming convenience, we use $W\\in\\mathbb{R}^{N\\times R}$ to replace $W\\in\\mathbb{R}^{R\\times N}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv as inv\n",
    "from numpy.random import normal as normrnd\n",
    "from scipy.linalg import khatri_rao as kr_prod\n",
    "from scipy.stats import wishart\n",
    "from scipy.stats import invwishart\n",
    "from numpy.linalg import solve as solve\n",
    "from numpy.linalg import cholesky as cholesky_lower\n",
    "from scipy.linalg import cholesky as cholesky_upper\n",
    "from scipy.linalg import solve_triangular as solve_ut\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mvnrnd_pre(mu, Lambda):\n",
    "    src = normrnd(size = (mu.shape[0],))\n",
    "    return solve_ut(cholesky_upper(Lambda, overwrite_a = True, check_finite = False), \n",
    "                    src, lower = False, check_finite = False, overwrite_b = True) + mu\n",
    "\n",
    "def cov_mat(mat, mat_bar):\n",
    "    mat = mat - mat_bar\n",
    "    return mat.T @ mat\n",
    "\n",
    "def ten2mat(tensor, mode):\n",
    "    return np.reshape(np.moveaxis(tensor, mode, 0), (tensor.shape[mode], -1), order = 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_factor_w(tau_sparse_mat, tau_ind, W, X, tau, beta0 = 1, vargin = 0):\n",
    "    \"\"\"Sampling N-by-R factor matrix W and its hyperparameters (mu_w, Lambda_w).\"\"\"\n",
    "    \n",
    "    dim1, rank = W.shape\n",
    "    W_bar = np.mean(W, axis = 0)\n",
    "    temp = dim1 / (dim1 + beta0)\n",
    "    var_mu_hyper = temp * W_bar\n",
    "    var_W_hyper = inv(np.eye(rank) + cov_mat(W, W_bar) + temp * beta0 * np.outer(W_bar, W_bar))\n",
    "    var_Lambda_hyper = wishart.rvs(df = dim1 + rank, scale = var_W_hyper)\n",
    "    var_mu_hyper = mvnrnd_pre(var_mu_hyper, (dim1 + beta0) * var_Lambda_hyper)\n",
    "    \n",
    "    if dim1 * rank ** 2 > 1e+8:\n",
    "        vargin = 1\n",
    "    \n",
    "    if vargin == 0:\n",
    "        var1 = X.T\n",
    "        var2 = kr_prod(var1, var1)\n",
    "        var3 = (var2 @ tau_ind.T).reshape([rank, rank, dim1]) + var_Lambda_hyper[:, :, None]\n",
    "        var4 = var1 @ tau_sparse_mat.T + (var_Lambda_hyper @ var_mu_hyper)[:, None]\n",
    "        for i in range(dim1):\n",
    "            W[i, :] = mvnrnd_pre(solve(var3[:, :, i], var4[:, i]), var3[:, :, i])\n",
    "    elif vargin == 1:\n",
    "        for i in range(dim1):\n",
    "            pos0 = np.where(sparse_mat[i, :] != 0)\n",
    "            Xt = X[pos0[0], :]\n",
    "            var_mu = tau[i] * Xt.T @ sparse_mat[i, pos0[0]] + var_Lambda_hyper @ var_mu_hyper\n",
    "            var_Lambda = tau[i] * Xt.T @ Xt + var_Lambda_hyper\n",
    "            W[i, :] = mvnrnd_pre(solve(var_Lambda, var_mu), var_Lambda)\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Sampling VAR Coefficients $A$ and Its Hyperparameters\n",
    "\n",
    "**Foundations of VAR**\n",
    "\n",
    "Vector autoregression (VAR) is a multivariate extension of autoregression (AR). Formally, VAR for $R$-dimensional vectors $\\boldsymbol{x}_{t}$ can be written as follows,\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{x}_{t}&=A_{1} \\boldsymbol{x}_{t-h_1}+\\cdots+A_{d} \\boldsymbol{x}_{t-h_d}+\\boldsymbol{\\epsilon}_{t}, \\\\\n",
    "&= A^\\top \\boldsymbol{v}_{t}+\\boldsymbol{\\epsilon}_{t},~t=h_d+1, \\ldots, T, \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "A=\\left[A_{1}, \\ldots, A_{d}\\right]^{\\top} \\in \\mathbb{R}^{(R d) \\times R},\\quad \\boldsymbol{v}_{t}=\\left[\\begin{array}{c}{\\boldsymbol{x}_{t-h_1}} \\\\ {\\vdots} \\\\ {\\boldsymbol{x}_{t-h_d}}\\end{array}\\right] \\in \\mathbb{R}^{(R d) \\times 1}.\n",
    "\\end{equation}\n",
    "\n",
    "In the following, if we define\n",
    "\\begin{equation}\n",
    "Z=\\left[\\begin{array}{c}{\\boldsymbol{x}_{h_d+1}^{\\top}} \\\\ {\\vdots} \\\\ {\\boldsymbol{x}_{T}^{\\top}}\\end{array}\\right] \\in \\mathbb{R}^{(T-h_d) \\times R},\\quad Q=\\left[\\begin{array}{c}{\\boldsymbol{v}_{h_d+1}^{\\top}} \\\\ {\\vdots} \\\\ {\\boldsymbol{v}_{T}^{\\top}}\\end{array}\\right] \\in \\mathbb{R}^{(T-h_d) \\times(R d)},\n",
    "\\end{equation}\n",
    "then, we could write the above mentioned VAR as\n",
    "\\begin{equation}\n",
    "\\underbrace{Z}_{(T-h_d)\\times R}\\approx \\underbrace{Q}_{(T-h_d)\\times (Rd)}\\times \\underbrace{A}_{(Rd)\\times R}.\n",
    "\\end{equation}\n",
    "\n",
    "> To include temporal factors $\\boldsymbol{x}_{t},t=1,...,h_d$, we also define $$Z_0=\\left[\\begin{array}{c}{\\boldsymbol{x}_{1}^{\\top}} \\\\ {\\vdots} \\\\ {\\boldsymbol{x}_{h_d}^{\\top}}\\end{array}\\right] \\in \\mathbb{R}^{h_d \\times R}.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a Bayesian VAR on temporal factors $\\boldsymbol{x}_{t}$**\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{x}_{t}&\\sim\\begin{cases}\\mathcal{N}\\left(A^\\top \\boldsymbol{v}_{t},\\Sigma\\right),~\\text{if $t\\in\\left\\{h_d+1,...,T\\right\\}$},\\\\{\\mathcal{N}\\left(\\boldsymbol{0},I_R\\right),~\\text{otherwise}}.\\end{cases}\\\\\n",
    "A&\\sim\\mathcal{MN}_{(Rd)\\times R}\\left(M_0,\\Psi_0,\\Sigma\\right), \\\\\n",
    "\\Sigma &\\sim\\mathcal{IW}\\left(S_0,\\nu_0\\right), \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&\\mathcal{M N}_{(R d) \\times R}\\left(A | M_{0}, \\Psi_{0}, \\Sigma\\right)\\\\\n",
    "\\propto|&\\Sigma|^{-R d / 2} \\exp \\left(-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}\\left(A-M_{0}\\right)^{\\top} \\Psi_{0}^{-1}\\left(A-M_{0}\\right)\\right]\\right), \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "\\mathcal{I} \\mathcal{W}\\left(\\Sigma | S_{0}, \\nu_{0}\\right) \\propto|\\Sigma|^{-\\left(\\nu_{0}+R+1\\right) / 2} \\exp \\left(-\\frac{1}{2} \\operatorname{tr}\\left(\\Sigma^{-1}S_{0}\\right)\\right).\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Likelihood from temporal factors $\\boldsymbol{x}_{t}$**\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&\\mathcal{L}\\left(X\\mid A,\\Sigma\\right) \\\\\n",
    "\\propto &\\prod_{t=1}^{h_d}p\\left(\\boldsymbol{x}_{t}\\mid \\Sigma\\right)\\times \\prod_{t=h_d+1}^{T}p\\left(\\boldsymbol{x}_{t}\\mid A,\\Sigma\\right) \\\\\n",
    "\\propto &\\left|\\Sigma\\right|^{-T/2}\\exp\\left\\{-\\frac{1}{2}\\sum_{t=h_d+1}^{T}\\left(\\boldsymbol{x}_{t}-A^\\top \\boldsymbol{v}_{t}\\right)^\\top\\Sigma^{-1}\\left(\\boldsymbol{x}_{t}-A^\\top \\boldsymbol{v}_{t}\\right)\\right\\} \\\\\n",
    "\\propto &\\left|\\Sigma\\right|^{-T/2}\\exp\\left\\{-\\frac{1}{2}\\text{tr}\\left[\\Sigma^{-1}\\left(Z_0^\\top Z_0+\\left(Z-QA\\right)^\\top \\left(Z-QA\\right)\\right)\\right]\\right\\}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Posterior distribution**\n",
    "\n",
    "Consider\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&\\left(A-M_{0}\\right)^{\\top} \\Psi_{0}^{-1}\\left(A-M_{0}\\right)+S_0+Z_0^\\top Z_0+\\left(Z-QA\\right)^\\top \\left(Z-QA\\right) \\\\\n",
    "=&A^\\top\\left(\\Psi_0^{-1}+Q^\\top Q\\right)A-A^\\top\\left(\\Psi_0^{-1}M_0+Q^\\top Z\\right) \\\\\n",
    "&-\\left(\\Psi_0^{-1}M_0+Q^\\top Z\\right)^\\top A \\\\\n",
    "&+\\left(\\Psi_0^{-1}M_0+Q^\\top Z\\right)^\\top\\left(\\Psi_0^{-1}+Q^\\top Q\\right)\\left(\\Psi_0^{-1}M_0+Q^\\top Z\\right) \\\\\n",
    "&-\\left(\\Psi_0^{-1}M_0+Q^\\top Z\\right)^\\top\\left(\\Psi_0^{-1}+Q^\\top Q\\right)\\left(\\Psi_0^{-1}M_0+Q^\\top Z\\right) \\\\\n",
    "&+M_0^\\top\\Psi_0^{-1}M_0+S_0+Z_0^\\top Z_0+Z^\\top Z \\\\\n",
    "=&\\left(A-M^{*}\\right)^\\top\\left(\\Psi^{*}\\right)^{-1}\\left(A-M^{*}\\right)+S^{*}, \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "which is in the form of $\\mathcal{MN}\\left(\\cdot\\right)$ and $\\mathcal{IW}\\left(\\cdot\\right)$.\n",
    "\n",
    "The $Rd$-by-$R$ matrix $A$ has a matrix normal distribution, and $R$-by-$R$ covariance matrix $\\Sigma$ has an inverse Wishart distribution, that is,\n",
    "\\begin{equation}\n",
    "A \\sim \\mathcal{M N}_{(R d) \\times R}\\left(M^{*}, \\Psi^{*}, \\Sigma\\right), \\quad \\Sigma \\sim \\mathcal{I} \\mathcal{W}\\left(S^{*}, \\nu^{*}\\right),\n",
    "\\end{equation}\n",
    "with\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "{\\Psi^{*}=\\left(\\Psi_{0}^{-1}+Q^{\\top} Q\\right)^{-1}}, \\\\ {M^{*}=\\Psi^{*}\\left(\\Psi_{0}^{-1} M_{0}+Q^{\\top} Z\\right)}, \\\\ {S^{*}=S_{0}+Z^\\top Z+M_0^\\top\\Psi_0^{-1}M_0-\\left(M^{*}\\right)^\\top\\left(\\Psi^{*}\\right)^{-1}M^{*}}, \\\\ \n",
    "{\\nu^{*}=\\nu_{0}+T-h_d}.\n",
    "\\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnrnd(M, U, V):\n",
    "    \"\"\"\n",
    "    Generate matrix normal distributed random matrix.\n",
    "    M is a m-by-n matrix, U is a m-by-m matrix, and V is a n-by-n matrix.\n",
    "    \"\"\"\n",
    "    dim1, dim2 = M.shape\n",
    "    X0 = np.random.randn(dim1, dim2)\n",
    "    P = cholesky_lower(U)\n",
    "    Q = cholesky_lower(V)\n",
    "    \n",
    "    return M + P @ X0 @ Q.T\n",
    "\n",
    "def sample_var_coefficient(X, time_lags):\n",
    "    dim, rank = X.shape\n",
    "    d = time_lags.shape[0]\n",
    "    tmax = np.max(time_lags)\n",
    "    \n",
    "    Z_mat = X[tmax : dim, :]\n",
    "    Q_mat = np.zeros((dim - tmax, rank * d))\n",
    "    for k in range(d):\n",
    "        Q_mat[:, k * rank : (k + 1) * rank] = X[tmax - time_lags[k] : dim - time_lags[k], :]\n",
    "    var_Psi0 = np.eye(rank * d) + Q_mat.T @ Q_mat\n",
    "    var_Psi = inv(var_Psi0)\n",
    "    var_M = var_Psi @ Q_mat.T @ Z_mat\n",
    "    var_S = np.eye(rank) + Z_mat.T @ Z_mat - var_M.T @ var_Psi0 @ var_M\n",
    "    Sigma = invwishart.rvs(df = rank + dim - tmax, scale = var_S)\n",
    "    \n",
    "    return mnrnd(var_M, var_Psi, Sigma), Sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Sampling Factor Matrix $X$\n",
    "\n",
    "**Posterior distribution**\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "y_{it}&\\sim\\mathcal{N}\\left(\\boldsymbol{w}_{i}^\\top\\boldsymbol{x}_{t},\\tau^{-1}\\right),~\\left(i,t\\right)\\in\\Omega, \\\\\n",
    "\\boldsymbol{x}_{t}&\\sim\\begin{cases}\\mathcal{N}\\left(\\sum_{k=1}^{d}A_{k} \\boldsymbol{x}_{t-h_k},\\Sigma\\right),~\\text{if $t\\in\\left\\{h_d+1,...,T\\right\\}$},\\\\{\\mathcal{N}\\left(\\boldsymbol{0},I\\right),~\\text{otherwise}}.\\end{cases}\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "If $t\\in\\left\\{1,...,h_d\\right\\}$, parameters of the posterior distribution $\\mathcal{N}\\left(\\boldsymbol{x}_{t}\\mid \\boldsymbol{\\mu}_{t}^{*},\\Sigma_{t}^{*}\\right)$ are\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\Sigma_{t}^{*}&=\\left(\\sum_{k=1, h_{d}<t+h_{k} \\leq T}^{d} {A}_{k}^{\\top} \\Sigma^{-1} A_{k}+\\tau\\sum_{i:(i,t)\\in\\Omega}\\boldsymbol{w}_{i}\\boldsymbol{w}_{i}^\\top+I\\right)^{-1}, \\\\\n",
    "\\boldsymbol{\\mu}_{t}^{*}&=\\Sigma_{t}^{*}\\left(\\sum_{k=1, h_{d}<t+h_{k} \\leq T}^{d} A_{k}^{\\top} \\Sigma^{-1} \\boldsymbol{\\psi}_{t+h_{k}}+\\tau\\sum_{i:(i,t)\\in\\Omega}\\boldsymbol{w}_{i}y_{it}\\right). \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "If $t\\in\\left\\{h_d+1,...,T\\right\\}$, then parameters of the posterior distribution $\\mathcal{N}\\left(\\boldsymbol{x}_{t}\\mid \\boldsymbol{\\mu}_{t}^{*},\\Sigma_{t}^{*}\\right)$ are\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\Sigma_{t}^{*}&=\\left(\\sum_{k=1, h_{d}<t+h_{k} \\leq T}^{d} {A}_{k}^{\\top} \\Sigma^{-1} A_{k}+\\tau\\sum_{i:(i,t)\\in\\Omega}\\boldsymbol{w}_{i}\\boldsymbol{w}_{i}^\\top+\\Sigma^{-1}\\right)^{-1}, \\\\\n",
    "\\boldsymbol{\\mu}_{t}^{*}&=\\Sigma_{t}^{*}\\left(\\sum_{k=1, h_{d}<t+h_{k} \\leq T}^{d} A_{k}^{\\top} \\Sigma^{-1} \\boldsymbol{\\psi}_{t+h_{k}}+\\tau\\sum_{i:(i,t)\\in\\Omega}\\boldsymbol{w}_{i}y_{it}+\\Sigma^{-1}\\sum_{k=1}^{d}A_{k}\\boldsymbol{x}_{t-h_k}\\right), \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where\n",
    "$$\\boldsymbol{\\psi}_{t+h_k}=\\boldsymbol{x}_{t+h_k}-\\sum_{l=1,l\\neq k}^{d}A_{l}\\boldsymbol{x}_{t+h_k-h_l}.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_factor_x(tau_sparse_mat, tau_ind, time_lags, W, X, A, Lambda_x):\n",
    "    \"\"\"Sampling T-by-R factor matrix X.\"\"\"\n",
    "    \n",
    "    dim2, rank = X.shape\n",
    "    tmax = np.max(time_lags)\n",
    "    tmin = np.min(time_lags)\n",
    "    d = time_lags.shape[0]\n",
    "    A0 = np.dstack([A] * d)\n",
    "    for k in range(d):\n",
    "        A0[k * rank : (k + 1) * rank, :, k] = 0\n",
    "    mat0 = Lambda_x @ A.T\n",
    "    mat1 = np.einsum('kij, jt -> kit', A.reshape([d, rank, rank]), Lambda_x)\n",
    "    mat2 = np.einsum('kit, kjt -> ij', mat1, A.reshape([d, rank, rank]))\n",
    "    \n",
    "    var1 = W.T\n",
    "    var2 = kr_prod(var1, var1)\n",
    "    var3 = (var2 @ tau_ind).reshape([rank, rank, dim2]) + Lambda_x[:, :, None]\n",
    "    var4 = var1 @ tau_sparse_mat\n",
    "    for t in range(dim2):\n",
    "        Mt = np.zeros((rank, rank))\n",
    "        Nt = np.zeros(rank)\n",
    "        Qt = mat0 @ X[t - time_lags, :].reshape(rank * d)\n",
    "        index = list(range(0, d))\n",
    "        if t >= dim2 - tmax and t < dim2 - tmin:\n",
    "            index = list(np.where(t + time_lags < dim2))[0]\n",
    "        elif t < tmax:\n",
    "            Qt = np.zeros(rank)\n",
    "            index = list(np.where(t + time_lags >= tmax))[0]\n",
    "        if t < dim2 - tmin:\n",
    "            Mt = mat2.copy()\n",
    "            temp = np.zeros((rank * d, len(index)))\n",
    "            n = 0\n",
    "            for k in index:\n",
    "                temp[:, n] = X[t + time_lags[k] - time_lags, :].reshape(rank * d)\n",
    "                n += 1\n",
    "            temp0 = X[t + time_lags[index], :].T - np.einsum('ijk, ik -> jk', A0[:, :, index], temp)\n",
    "            Nt = np.einsum('kij, jk -> i', mat1[index, :, :], temp0)\n",
    "        \n",
    "        var3[:, :, t] = var3[:, :, t] + Mt\n",
    "        if t < tmax:\n",
    "            var3[:, :, t] = var3[:, :, t] - Lambda_x + np.eye(rank)\n",
    "        X[t, :] = mvnrnd_pre(solve(var3[:, :, t], var4[:, t] + Nt + Qt), var3[:, :, t])\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Sampling Precision $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_precision_tau(sparse_mat, mat_hat, ind):\n",
    "    var_alpha = 1e-6 + 0.5 * np.sum(ind, axis = 1)\n",
    "    var_beta = 1e-6 + 0.5 * np.sum(((sparse_mat - mat_hat) ** 2) * ind, axis = 1)\n",
    "    return np.random.gamma(var_alpha, 1 / var_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mape(var, var_hat):\n",
    "    return np.sum(np.abs(var - var_hat) / var) / var.shape[0]\n",
    "\n",
    "def compute_rmse(var, var_hat):\n",
    "    return  np.sqrt(np.sum((var - var_hat) ** 2) / var.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) BTMF Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter, multi_steps = 1):\n",
    "    \"\"\"Bayesian Temporal Matrix Factorization, BTMF.\"\"\"\n",
    "    \n",
    "    dim1, dim2 = sparse_mat.shape\n",
    "    d = time_lags.shape[0]\n",
    "    W = init[\"W\"]\n",
    "    X = init[\"X\"]\n",
    "    pos_test = np.where((dense_mat != 0) & (sparse_mat == 0))\n",
    "    dense_test = dense_mat[pos_test]\n",
    "    del dense_mat\n",
    "    ind = sparse_mat != 0\n",
    "    pos_obs = np.where(ind)\n",
    "    tau = np.ones(dim1)\n",
    "    W_plus = np.zeros((dim1, rank))\n",
    "    X_new_plus = np.zeros((dim2 + multi_steps, rank))\n",
    "    A_plus = np.zeros((rank * d, rank))\n",
    "    temp_hat = np.zeros(len(pos_test[0]))\n",
    "    show_iter = 200\n",
    "    mat_hat_plus = np.zeros((dim1, dim2))\n",
    "    for it in range(burn_iter + gibbs_iter):\n",
    "        tau_ind = tau[:, None] * ind\n",
    "        tau_sparse_mat = tau[:, None] * sparse_mat\n",
    "        W = sample_factor_w(tau_sparse_mat, tau_ind, W, X, tau, beta0 = 1, vargin = 0)\n",
    "        A, Sigma = sample_var_coefficient(X, time_lags)\n",
    "        X = sample_factor_x(tau_sparse_mat, tau_ind, time_lags, W, X, A, inv(Sigma))\n",
    "        mat_hat = W @ X.T\n",
    "        tau = sample_precision_tau(sparse_mat, mat_hat, ind)\n",
    "        temp_hat += mat_hat[pos_test]\n",
    "        if (it + 1) % show_iter == 0 and it < burn_iter:\n",
    "            temp_hat = temp_hat / show_iter\n",
    "            print('Iter: {}'.format(it + 1))\n",
    "            print('MAPE: {:.6}'.format(compute_mape(dense_test, temp_hat)))\n",
    "            print('RMSE: {:.6}'.format(compute_rmse(dense_test, temp_hat)))\n",
    "            temp_hat = np.zeros(len(pos_test[0]))\n",
    "            print()\n",
    "        X_new = np.zeros((dim2 + multi_steps, rank))\n",
    "        if it + 1 > burn_iter:\n",
    "            W_plus += W\n",
    "            A_plus += A\n",
    "            X_new[: dim2, :] = X.copy()\n",
    "            if multi_steps == 1:\n",
    "                X_new[dim2, :] = A.T @ X_new[dim2 - time_lags, :].reshape(rank * d)\n",
    "            elif multi_steps > 1:\n",
    "                for t0 in range(multi_steps):\n",
    "                    X_new[dim2 + t0, :] = A.T @ X_new[dim2 + t0 - time_lags, :].reshape(rank * d)\n",
    "            X_new_plus += X_new\n",
    "            mat_hat_plus += mat_hat\n",
    "    mat_hat = mat_hat_plus / gibbs_iter\n",
    "    W = W_plus / gibbs_iter\n",
    "    X_new = X_new_plus / gibbs_iter\n",
    "    A = A_plus / gibbs_iter\n",
    "    print('Imputation MAPE: {:.6}'.format(compute_mape(dense_test, mat_hat[pos_test])))\n",
    "    print('Imputation RMSE: {:.6}'.format(compute_rmse(dense_test, mat_hat[pos_test])))\n",
    "    print()\n",
    "    \n",
    "    return mat_hat, W, X_new, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PeMS-4W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1000)\n",
    "\n",
    "data = pd.read_csv('../datasets/California-data-set/pems-4w.csv', header = None)\n",
    "dense_mat = data.values\n",
    "random_mat = ten2mat(np.random.rand(data.values.shape[0], 288, 4 * 7), 0)\n",
    "del data\n",
    "\n",
    "missing_rate = 0.3\n",
    "\n",
    "### Random missing (RM) scenario:\n",
    "sparse_mat = np.multiply(dense_mat, np.round(random_mat + 0.5 - missing_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 10\n",
    "time_lags = np.array([1, 2, 288])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X_new, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %.2f minutes'%((end - start)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1000)\n",
    "\n",
    "data = pd.read_csv('../datasets/California-data-set/pems-4w.csv', header = None)\n",
    "dense_mat = data.values\n",
    "random_mat = ten2mat(np.random.rand(data.values.shape[0], 288, 4 * 7), 0)\n",
    "del data\n",
    "\n",
    "missing_rate = 0.7\n",
    "\n",
    "### Random missing (RM) scenario:\n",
    "sparse_mat = np.multiply(dense_mat, np.round(random_mat + 0.5 - missing_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 10\n",
    "time_lags = np.array([1, 2, 288])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X_new, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %.2f minutes'%((end - start)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1000)\n",
    "\n",
    "data = pd.read_csv('../datasets/California-data-set/pems-4w.csv', header = None)\n",
    "dense_mat = data.values\n",
    "dense_tensor = mat2ten(dense_mat, np.array([data.values.shape[0], 288, 4 * 7]), 0)\n",
    "random_matrix = np.random.rand(data.values.shape[0], 4 * 7)\n",
    "\n",
    "missing_rate = 0.3\n",
    "\n",
    "### Non-random missing (NM) scenario:\n",
    "binary_tensor = np.zeros(dense_tensor.shape)\n",
    "for i1 in range(dense_tensor.shape[0]):\n",
    "    for i2 in range(dense_tensor.shape[2]):\n",
    "        binary_tensor[i1, :, i2] = np.round(random_matrix[i1, i2] + 0.5 - missing_rate)\n",
    "sparse_mat = ten2mat(np.multiply(dense_tensor, binary_tensor), 0)\n",
    "del data, dense_tensor, binary_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 10\n",
    "time_lags = np.array([1, 2, 288])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X_new, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %.2f minutes'%((end - start)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1000)\n",
    "\n",
    "data = pd.read_csv('../datasets/California-data-set/pems-4w.csv', header = None)\n",
    "dense_mat = data.values\n",
    "dense_tensor = mat2ten(dense_mat, np.array([data.values.shape[0], 288, 4 * 7]), 0)\n",
    "random_matrix = np.random.rand(data.values.shape[0], 4 * 7)\n",
    "\n",
    "missing_rate = 0.7\n",
    "\n",
    "### Non-random missing (NM) scenario:\n",
    "binary_tensor = np.zeros(dense_tensor.shape)\n",
    "for i1 in range(dense_tensor.shape[0]):\n",
    "    for i2 in range(dense_tensor.shape[2]):\n",
    "        binary_tensor[i1, :, i2] = np.round(random_matrix[i1, i2] + 0.5 - missing_rate)\n",
    "sparse_mat = ten2mat(np.multiply(dense_tensor, binary_tensor), 0)\n",
    "del data, dense_tensor, binary_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 10\n",
    "time_lags = np.array([1, 2, 288])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X_new, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %.2f minutes'%((end - start)/60.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PeMS-8W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1000)\n",
    "\n",
    "data = pd.read_csv('../datasets/California-data-set/pems-8w.csv', header = None)\n",
    "dense_mat = data.values\n",
    "random_mat = ten2mat(np.random.rand(data.values.shape[0], 288, 8 * 7), 0)\n",
    "del data\n",
    "\n",
    "missing_rate = 0.3\n",
    "\n",
    "### Random missing (RM) scenario:\n",
    "sparse_mat = np.multiply(dense_mat, np.round(random_mat + 0.5 - missing_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 10\n",
    "time_lags = np.array([1, 2, 288])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X_new, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %.2f minutes'%((end - start)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1000)\n",
    "\n",
    "data = pd.read_csv('../datasets/California-data-set/pems-8w.csv', header = None)\n",
    "dense_mat = data.values\n",
    "random_mat = ten2mat(np.random.rand(data.values.shape[0], 288, 8 * 7), 0)\n",
    "del data\n",
    "\n",
    "missing_rate = 0.7\n",
    "\n",
    "### Random missing (RM) scenario:\n",
    "sparse_mat = np.multiply(dense_mat, np.round(random_mat + 0.5 - missing_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 10\n",
    "time_lags = np.array([1, 2, 288])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X_new, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %.2f minutes'%((end - start)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1000)\n",
    "\n",
    "data = pd.read_csv('../datasets/California-data-set/pems-8w.csv', header = None)\n",
    "dense_mat = data.values\n",
    "dense_tensor = mat2ten(dense_mat, np.array([data.values.shape[0], 288, 8 * 7]), 0)\n",
    "random_matrix = np.random.rand(data.values.shape[0], 8 * 7)\n",
    "\n",
    "missing_rate = 0.3\n",
    "\n",
    "### Non-random missing (NM) scenario:\n",
    "binary_tensor = np.zeros(dense_tensor.shape)\n",
    "for i1 in range(dense_tensor.shape[0]):\n",
    "    for i2 in range(dense_tensor.shape[2]):\n",
    "        binary_tensor[i1, :, i2] = np.round(random_matrix[i1, i2] + 0.5 - missing_rate)\n",
    "sparse_mat = ten2mat(np.multiply(dense_tensor, binary_tensor), 0)\n",
    "del data, dense_tensor, binary_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 10\n",
    "time_lags = np.array([1, 2, 288])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X_new, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %.2f minutes'%((end - start)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1000)\n",
    "\n",
    "data = pd.read_csv('../datasets/California-data-set/pems-8w.csv', header = None)\n",
    "dense_mat = data.values\n",
    "dense_tensor = mat2ten(dense_mat, np.array([data.values.shape[0], 288, 8 * 7]), 0)\n",
    "random_matrix = np.random.rand(data.values.shape[0], 8 * 7)\n",
    "\n",
    "missing_rate = 0.7\n",
    "\n",
    "### Non-random missing (NM) scenario:\n",
    "binary_tensor = np.zeros(dense_tensor.shape)\n",
    "for i1 in range(dense_tensor.shape[0]):\n",
    "    for i2 in range(dense_tensor.shape[2]):\n",
    "        binary_tensor[i1, :, i2] = np.round(random_matrix[i1, i2] + 0.5 - missing_rate)\n",
    "sparse_mat = ten2mat(np.multiply(dense_tensor, binary_tensor), 0)\n",
    "del data, dense_tensor, binary_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 10\n",
    "time_lags = np.array([1, 2, 288])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X_new, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %.2f minutes'%((end - start)/60.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# London 1-M data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "missing_rate = 0.3\n",
    "\n",
    "dense_mat = np.load('../datasets/London-data-set/hourly_speed_mat.npy')\n",
    "binary_mat = dense_mat.copy()\n",
    "binary_mat[binary_mat != 0] = 1\n",
    "pos = np.where(np.sum(binary_mat, axis = 1) > 0.7 * binary_mat.shape[1])\n",
    "dense_mat = dense_mat[pos[0], :]\n",
    "\n",
    "## Random missing (RM)\n",
    "random_mat = np.random.rand(dense_mat.shape[0], dense_mat.shape[1])\n",
    "binary_mat = np.round(random_mat + 0.5 - missing_rate)\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 20\n",
    "time_lags = np.array([1, 2, 24])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X_new, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "missing_rate = 0.7\n",
    "\n",
    "dense_mat = np.load('../datasets/London-data-set/hourly_speed_mat.npy')\n",
    "binary_mat = dense_mat.copy()\n",
    "binary_mat[binary_mat != 0] = 1\n",
    "pos = np.where(np.sum(binary_mat, axis = 1) > 0.7 * binary_mat.shape[1])\n",
    "dense_mat = dense_mat[pos[0], :]\n",
    "\n",
    "## Random missing (RM)\n",
    "random_mat = np.random.rand(dense_mat.shape[0], dense_mat.shape[1])\n",
    "binary_mat = np.round(random_mat + 0.5 - missing_rate)\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 20\n",
    "time_lags = np.array([1, 2, 24])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X_new, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "missing_rate = 0.3\n",
    "\n",
    "dense_mat = np.load('../datasets/London-data-set/hourly_speed_mat.npy')\n",
    "binary_mat = dense_mat.copy()\n",
    "binary_mat[binary_mat != 0] = 1\n",
    "pos = np.where(np.sum(binary_mat, axis = 1) > 0.7 * binary_mat.shape[1])\n",
    "dense_mat = dense_mat[pos[0], :]\n",
    "\n",
    "## Non-random missing (NM)\n",
    "binary_mat = np.zeros(dense_mat.shape)\n",
    "random_mat = np.random.rand(dense_mat.shape[0], 30)\n",
    "for i1 in range(dense_mat.shape[0]):\n",
    "    for i2 in range(30):\n",
    "        binary_mat[i1, i2 * 24 : (i2 + 1) * 24] = np.round(random_mat[i1, i2] + 0.5 - missing_rate)\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 20\n",
    "time_lags = np.array([1, 2, 24])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X_new, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "missing_rate = 0.7\n",
    "\n",
    "dense_mat = np.load('../datasets/London-data-set/hourly_speed_mat.npy')\n",
    "binary_mat = dense_mat.copy()\n",
    "binary_mat[binary_mat != 0] = 1\n",
    "pos = np.where(np.sum(binary_mat, axis = 1) > 0.7 * binary_mat.shape[1])\n",
    "dense_mat = dense_mat[pos[0], :]\n",
    "\n",
    "## Non-random missing (NM)\n",
    "binary_mat = np.zeros(dense_mat.shape)\n",
    "random_mat = np.random.rand(dense_mat.shape[0], 30)\n",
    "for i1 in range(dense_mat.shape[0]):\n",
    "    for i2 in range(30):\n",
    "        binary_mat[i1, i2 * 24 : (i2 + 1) * 24] = np.round(random_mat[i1, i2] + 0.5 - missing_rate)\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 20\n",
    "time_lags = np.array([1, 2, 24])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X_new, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guangzhou 2-M data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/tensor.mat')['tensor']\n",
    "random_tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/random_tensor.mat')['random_tensor']\n",
    "dense_mat = tensor.reshape([tensor.shape[0], tensor.shape[1] * tensor.shape[2]])\n",
    "missing_rate = 0.3\n",
    "\n",
    "## Random missing (RM)\n",
    "binary_mat = (np.round(random_tensor + 0.5 - missing_rate)\n",
    "              .reshape([random_tensor.shape[0], random_tensor.shape[1] * random_tensor.shape[2]]))\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 80\n",
    "time_lags = np.array([1, 2, 144])\n",
    "init = {\"W\": 0.01 * np.random.randn(dim1, rank), \"X\": 0.01 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X_new, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/tensor.mat')['tensor']\n",
    "random_tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/random_tensor.mat')['random_tensor']\n",
    "dense_mat = tensor.reshape([tensor.shape[0], tensor.shape[1] * tensor.shape[2]])\n",
    "missing_rate = 0.7\n",
    "\n",
    "## Random missing (RM)\n",
    "binary_mat = (np.round(random_tensor + 0.5 - missing_rate)\n",
    "              .reshape([random_tensor.shape[0], random_tensor.shape[1] * random_tensor.shape[2]]))\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 80\n",
    "time_lags = np.array([1, 2, 144])\n",
    "init = {\"W\": 0.01 * np.random.randn(dim1, rank), \"X\": 0.01 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X_new, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/tensor.mat')['tensor']\n",
    "random_matrix = scipy.io.loadmat('../datasets/Guangzhou-data-set/random_matrix.mat')['random_matrix']\n",
    "dense_mat = tensor.reshape([tensor.shape[0], tensor.shape[1] * tensor.shape[2]])\n",
    "missing_rate = 0.3\n",
    "\n",
    "## Non-random missing (NM)\n",
    "binary_tensor = np.zeros(tensor.shape)\n",
    "for i1 in range(tensor.shape[0]):\n",
    "    for i2 in range(tensor.shape[1]):\n",
    "        binary_tensor[i1, i2, :] = np.round(random_matrix[i1, i2] + 0.5 - missing_rate)\n",
    "binary_mat = binary_tensor.reshape([binary_tensor.shape[0], binary_tensor.shape[1] * binary_tensor.shape[2]])\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 10\n",
    "time_lags = np.array([1, 2, 144])\n",
    "init = {\"W\": 0.01 * np.random.randn(dim1, rank), \"X\": 0.01 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X_new, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/tensor.mat')['tensor']\n",
    "random_matrix = scipy.io.loadmat('../datasets/Guangzhou-data-set/random_matrix.mat')['random_matrix']\n",
    "dense_mat = tensor.reshape([tensor.shape[0], tensor.shape[1] * tensor.shape[2]])\n",
    "missing_rate = 0.7\n",
    "\n",
    "## Non-random missing (NM)\n",
    "binary_tensor = np.zeros(tensor.shape)\n",
    "for i1 in range(tensor.shape[0]):\n",
    "    for i2 in range(tensor.shape[1]):\n",
    "        binary_tensor[i1, i2, :] = np.round(random_matrix[i1, i2] + 0.5 - missing_rate)\n",
    "binary_mat = binary_tensor.reshape([binary_tensor.shape[0], binary_tensor.shape[1] * binary_tensor.shape[2]])\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 10\n",
    "time_lags = np.array([1, 2, 144])\n",
    "init = {\"W\": 0.01 * np.random.randn(dim1, rank), \"X\": 0.01 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X_new, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>This work is released under the MIT license.</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
